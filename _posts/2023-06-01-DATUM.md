---
layout: post
title: One-shot Unsupervised Data Augmentation
date: 2023-04-19 16:11:00-0400
inline: false
related_posts: false
---
DATUM: <a href="https://arxiv.org/pdf/2303.18080.pdf">One-shot Unsupervised Domain Adaptation with Personalized Diffusion Models</a> <br>
UniDiffuser: <a href="https://arxiv.org/pdf/2303.06555.pdf">One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</a> <br>
I2SB: <a href="https://arxiv.org/pdf/2303.16852.pdf">I<sup>2</sup>SB: Image-to-Image Schr√∂dinger Bridge</a> <br>
DDPM_inversion: <a href="https://arxiv.org/pdf/2304.06140.pdf">An Edit Friendly DDPM Noise Space: Inversion and Manipulations</a>
<!-- 
<a href="#top">(DATUM, 2023)</a>
<a href="#top">(UniDiffuser, 2023)</a> 
<a href="#top">(I2SB, 2023)</a> 
<a href="#top">(DDPM_inversion, 2023)</a> 
-->

***

The article <a href="#top">(DATUM, 2023)</a> focuses on the challenging problem of adapting a segmentation model from a labeled source domain to a target domain with only a single unlabeled datum available. This problem is commonly known as one-shot unsupervised domain adaptation (OSUDA).

While previous approaches have relied on style transfer techniques to address OSUDA, the authors propose a novel method called DATUM that leverages text-to-image diffusion models, e.g. StableDiffusion or DreamBooth. This approach generates a synthetic target dataset comprising photo-realistic images that not only capture the style of the target domain but also feature novel scenes in diverse contexts. Unlike existing OSUDA methods, DATUM incorporates a text interface that allows guiding the image generation process towards desired semantic concepts while preserving the spatial context of a single training image. 

In this blog, we aim to shine a spotlight on a key factor that influences the performance and impact of the prompt in DATUM. As we currently know, prompt engineering involves designing and refining the textual prompts used to guide the image generation process in text-to-image models. This seemingly small yet critical aspect plays a pivotal role in unleashing the full potential of DATUM and maximizing its effectiveness in domain adaptation. We will discuss how the choice of prompt can influence the style transfer, semantic concepts, and spatial context of the generated images. By understanding the nuances of prompt engineering, researchers and practitioners can exploit DATUM to align with their specific domain adaptation needs and achieve superior results.

***

<!-- Yasser Benigmim et al. propose to fine-tune DreamBooth with the prompt: "A photo of sks urban scene". -->
Yasser Benigmim et al. put forward the approach of fine-tuning the DreamBooth model using the specific prompt: "A photo of sks urban scene." In the first part, our experimental setup revolves around conducting tests and investigations on the following image associated with the prompt "A photo of sks urban scene" as used in Benigmim et al. <a href="#top">(DATUM, 2023)</a>. Also, the training configuration is kept to the default settings unless otherwise specified.
<!-- ![image](/assets/img/news/DATUM/sks-urban-scene.jpg){width=50%} -->
<div>
    <img src="/assets/img/news/DATUM/sks-urban-scene.jpg" width="100%">
    <div class="img-cap">
        <b>Figure 1.</b> Training prompt: A photo of <b>sks</b> urban scene
    </div>
</div>

<!-- We will make experiments on the this image to answer the corresponding questions in below. The training configuration remains as default.<span id="q1"></span> -->

#### 1. What will happen if the training prompt does not indicate exactly the style of the input ?

The investigation aims to assess the impact of invalid training prompts on the quality of the fine-tuned model. Our findings confirm that such prompts indeed have a detrimental effect. Specifically, when considering the class "rainy," the majority of the generated images fail to depict rain accurately. In the case of the classes "snowy" and "sunny," although the outputs appear satisfactory overall, noticeable deficiencies arise with respect to the representation of people and cars, as these elements tend to be diminished or absent.

<!-- We would like to see if the invalid training prompt affects the quality of the fine-tuned model. The answer is yes. Most of the generated images with respect to the class rainy do not contain the rain. For the classes <i>snowy</i> and <i>sunny</i>, the outputs seems good but people and cars almost disappear. -->

<div>
    <div>
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_rainy_day.png" width="49%">
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_rainy_day_sky.png" width="49%">
        <div class="img-cap">
            <b>Figure 2.</b> Test prompt: A photo of <b>sks</b> urban scene on a <i>rainy</i> day. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 200 iterations
        </div>
    </div>
</div>

<div>
    <div>
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_sunny_day.png" width="49%">
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_sunny_day_sky.png" width="49%">
        <div class="img-cap">
            <b>Figure 3.</b> Test prompt: A photo of <b>sks</b> urban scene on a <i>sunny</i> day. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 200 iterations
        </div>
    </div>
</div>

<div>
    <div>
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_snowy_day.png" width="49%">
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_snowy_day_sky.png" width="49%">
        <div class="img-cap">
            <b>Figure 4.</b> Test prompt: A photo of <b>sks</b> urban scene on a <i>snowy</i> day. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 200 iterations
        </div>
    </div>
</div>

These results underscore the significance of carefully selecting the training prompts during the fine-tuning process of the DreamBooth model. They align with the conclusions drawn in the ablation study conducted by Benigmim et al. <a href="#top">(DATUM, 2023)</a>, which also highlights the influence of training prompts on segmentation outcomes. However, it is worth noting that the endeavor of prompt engineering entails considerable effort in order to identify an optimal training text that yields desirable results in image sampling. Fortunately, Bao et al. have introduced a solution <a href="#top">(UniDiffuser, 2023)</a> that leverages a variant of the transformer model. This approach can be effectively employed to deduce an appropriate prompt for input images in text-to-image models such as StableDiffusion and DreamBooth.

 <!-- These results show that when fine-tuning DreamBooth, we should select the training prompt carefully otherwise we end up with unexpected cases. It is also shown in the ablation study by Benigmim et al. <a href="#top">(DATUM, 2023)</a> on how the training prompts impact the segmentation results. However, it requires a huge effort on prompt engineering to actually choose an optimal training text that performs well on sampling. Fortunately, Bao et al. propose <a href="#top">(UniDiffuser, 2023)</a> which employs a variation of transformer and can be exploited to infer a correct prompt of an input image for text-to-image models as StableDiffusion and DreamBooth.  -->

#### 2. How does the model work if the classes are not relevant to the style ?
<!-- When the classes are not relevant to the style (e.g. "paddy field", "moutain"), the fine-tuned model with the correct prompt is still able to generate valid images while the one with invalid prompt loses its generalized capability. -->
In scenarios where the targeted classes do not align with the intended style (e.g., "paddy field" or "mountain"), it is observed that the fine-tuned model incorporating the correct prompt retains its ability to generate valid images. Conversely, the model utilizing an invalid prompt demonstrates a diminished capacity for generalization.

<div>
    <div>
        <img src="/assets/img/news/DATUM/a_photo_of_sks_class.png" width="49%">
        <img src="/assets/img/news/DATUM/a_photo_of_sks_class_sky.png" width="49%">
        <div class="img-cap">
            <b>Figure 5.</b> Test prompt: A photo of <b>sks</b> &lt;class&gt;.  &lt;class&gt; from left to right: urban scene - paddy field - mountain - sky. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 200 iterations
        </div>
    </div>
</div>
<!-- The results in Figure 5 show that if we would like to sample images that are not really related to the style, e.g. <span style="color:red">paddy field</span> and <span style="color:red">mountain</span> versus <span style="color:cyan">sky</span> and <span style="color:cyan">urban scene</span>, we are more likely to get the images relevant to the training prompt than the test prompt. Hence, the prompt is sensitive not only for the style (fine-tuning phase) as in Question <a href="#q1">1</a> above but also for the target (testing phase) that we opt to. We notice that for users to attentively input prompts and use valid fine-tuned models. -->
Figure 5 depicts the results indicating that when attempting to sample images unrelated to the style, such as <span style="color:red">paddy field</span> and <span style="color:red">mountain</span> compared to <span style="color:cyan">sky</span> and <span style="color:cyan">urban scene</span>, the generated images are more likely to correspond to the training prompt rather than the desired test prompt. This observation underscores the sensitivity of the prompt, not only in the fine-tuning phase concerning style (as addressed in Question <a href="#q1">1</a>), but also in the testing phase regarding the specific target prompt chosen. Consequently, it is imperative for users to exercise caution when providing prompts and ensure the utilization of valid and appropriate fine-tuned models to obtain desired outcomes.

***

#### 3. Try to synthesize images in different weather conditions
<div>
    <img src="/assets/img/news/DATUM/sks-urban-scene-rainy-day.jpg" width="100%">
    <div class="img-cap">
        <b>Figure 6.</b> Training prompt: A photo of <b>sks</b> urban scene on a rainy day
    </div>
</div>
<!-- Training the following image under rainy condition. -->
We expand our experiments on a different image as illustrated in Figure 6, where the weather condition "rainy" is given. We would like to expect if we can replace "rainy" by other conditions as "sunny" and "snowy". Additionally, we intend to explore the synthesis of images depicting different weather conditions by modifying the number of iterations during the training phase, specifically employing 200 and 1000 iterations. This modification aims to investigate whether, over time, we can generate images of the same scene with varying weather conditions. By incorporating these adjustments, we aspire to gain further insights into the behavior and capabilities of the model within different experimental scenarios.
<div>
    <img src="/assets/img/news/DATUM/sks-urban-scene-weather-200.png" width="100%">
    <div class="img-cap">
        <b>Figure 7.</b> Test prompt: A photo of <b>sks</b> &lt;class&gt;.  &lt;class&gt; from top to bottom: sunny - rainy - snowy. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 200 iterations
    </div>
</div>
<!-- The results in Figure 7 show that we can definitely use DATUM to adapt the images in "rainy" to "sunny", "snowy" or different style of "rainy". However, we can also see that the fine-tuned model learns the piles as a style. -->
The findings presented in Figure 7 definitely demonstrate the effectiveness of DATUM in facilitating the adaptation of images from the "rainy" domain to alternative styles such as "sunny", "snowy", or even varying manifestations of the "rainy" scene. Nevertheless, an intriguing observation is made, revealing that the fine-tuned model exhibits a propensity for treating the visual concept of "piles" as an inherent stylistic attribute. One can try to fine-tune DreamBooth in smaller numbers of iterations to see if "piles" disappear without negatively affect the different weather conditions synthesis.
<div>
    <img src="/assets/img/news/DATUM/sks-urban-scene-weather-1000.png" width="100%">
    <div class="img-cap">
        <b>Figure 8.</b> Test prompt: A photo of <b>sks</b> &lt;class&gt;.  &lt;class&gt; from top to bottom: sunny - rainy - snowy. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 1000 iterations
    </div>
</div>
<!-- When train with 1000 iterations, the model overfits and cannot generate images with respect to <i>sunny</i> and <i>snowy</i>. Moreover, the <i>rainy</i>, <i>piles</i>, <i>purple color</i>, <i>people riding bikes with raincoat</i>, etc. are also considered to be the style. -->
When the model is trained using 1000 iterations, a phenomenon of overfitting is observed, resulting in the model's inability to generate images accurately corresponding to the classes of "sunny" and "snowy". Furthermore, it is worth noting that the attributes of "rainy," "piles," "purple color," "people riding bikes with raincoat," and others are also deemed stylistic elements within the training framework.

***

In conclusion, the article (DATUM, 2023) addresses the challenging problem of one-shot unsupervised domain adaptation (OSUDA) in the context of segmenting images. The authors propose DATUM as a novel approach that leverages text-to-image diffusion models, such as StableDiffusion or DreamBooth, to generate a synthetic target dataset that faithfully captures the style of the target domain while incorporating novel scenes in diverse contexts. The selection of appropriate training prompts plays a crucial role in the performance of the fine-tuned model. The results highlight the need for careful prompt engineering and the potential of prompt-guided image generation techniques like UniDiffuser to enhance the effectiveness of DATUM. Furthermore, the experiments demonstrate that the choice of prompt is not only crucial during the fine-tuning phase but also during the testing phase, as the generated images are more likely to align with the training prompt rather than the desired target prompt when the classes are unrelated to the style.

Expanding the experiments, the synthesis of images with different weather conditions reveals interesting insights. The fine-tuned model exhibits the ability to adapt images from the "rainy" domain to alternative styles such as "sunny" and "snowy." We can further use those generated images in training another models that perform distribution shift. Specifically, those synthesis data can be considered as pairs of classes ("rainy", "sunny") to train a model, e.g. <a href="#top">(I2SB, 2023)</a>, to adapt rainy&rarr;sunny or snowy&rarr;sunny tasks which then strongly support autonomous driving. However, every pair of images requires the scenes in two images nearly the same and the only difference is the weather conditions. This is an issue remaining in most state-of-the-art variations of (guidance) generative models, as we can see in the discussion above <a href="#top">(I2SB, 2023)</a>. Also, in the following figure <a href="#top">(DDPM_inversion, 2023)</a>, the signboard and street light disappeared in rainy and snowy condition, respectively.

<div>
    <img src="/assets/img/news/DATUM/sunny-rainy-snowy.png" width="100%">
    <div class="img-cap">
        <b>Figure 9.</b> Synthesis images generated by <a href="#top">(DDPM_inversion, 2023)</a> with the prompts generated by <a href="#top">(UniDiffuser, 2023)</a> in different weather conditions. From left to right: sunny - rainy - snowy. 
    </div>
</div>