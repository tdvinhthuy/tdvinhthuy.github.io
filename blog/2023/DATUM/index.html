<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>One-shot Unsupervised Data Augmentation | Thuy Vinh Dinh Tran</title>
    <meta name="author" content="Thuy Vinh Dinh Tran">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.`.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://tdvinhthuy.github.io/blog/2023/DATUM/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Thuy </span>Vinh Dinh Tran</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">Blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">Publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- _layouts/post.html -->

<div class="post">

  <header class="post-header">
    <h1 class="post-title">One-shot Unsupervised Data Augmentation</h1>
    <p class="post-meta">April 19, 2023</p>
    <p class="post-tags">
      <a href="/blog/2023"> <i class="fas fa-calendar fa-sm"></i> 2023 </a>

    </p>
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <p>DATUM: <a href="https://arxiv.org/pdf/2303.18080.pdf" rel="external nofollow noopener" target="_blank">One-shot Unsupervised Domain Adaptation with Personalized Diffusion Models</a><br>
UniDiffuser: <a href="https://arxiv.org/pdf/2303.06555.pdf" rel="external nofollow noopener" target="_blank">One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</a>
<!-- 
<a href="#top">(DATUM, 2023)</a>
<a href="#top">(UniDiffuser, 2023)</a> 
--></p>

<hr>

<p>The article <a href="#top">(DATUM, 2023)</a> focuses on the challenging problem of adapting a segmentation model from a labeled source domain to a target domain with only a single unlabeled datum available. This problem is commonly known as one-shot unsupervised domain adaptation (OSUDA).</p>

<p>While previous approaches have relied on style transfer techniques to address OSUDA, the authors propose a novel method called <a href="#top">(DATUM, 2023)</a> that leverages text-to-image diffusion models, e.g. StableDiffusion or DreamBooth. This approach generates a synthetic target dataset comprising photo-realistic images that not only capture the style of the target domain but also feature novel scenes in diverse contexts. Unlike existing OSUDA methods, DATUM incorporates a text interface that allows guiding the image generation process towards desired semantic concepts while preserving the spatial context of a single training image.</p>

<p>In this blog, we aim to shine a spotlight on a key factor that influences the performance and impact of the prompt in DATUM. As we currently know, prompt engineering involves designing and refining the textual prompts used to guide the image generation process in text-to-image models. This seemingly small yet critical aspect plays a pivotal role in unleashing the full potential of DATUM and maximizing its effectiveness in domain adaptation. We will discuss how the choice of prompt can influence the style transfer, semantic concepts, and spatial context of the generated images. By understanding the nuances of prompt engineering, researchers and practitioners can exploit DATUM to align with their specific domain adaptation needs and achieve superior results.</p>

<hr>

<!-- Yasser Benigmim et al. propose to fine-tune DreamBooth with the prompt: "A photo of sks urban scene". -->
<p>Yasser Benigmim et al. put forward the approach of fine-tuning the DreamBooth model using the specific prompt: “A photo of sks urban scene.” In the first part, our experimental setup revolves around conducting tests and investigations on the following image associated with the prompt “A photo of sks urban scene” as used in Benigmim et al. <a href="#top">(DATUM, 2023)</a>. Also, the training configuration is kept to the default settings unless otherwise specified.
<!-- ![image](/assets/img/news/DATUM/sks-urban-scene.jpg){width=50%} --></p>
<div>
    <img src="/assets/img/news/DATUM/sks-urban-scene.jpg" width="100%">
    <div class="img-cap">
        <b>Figure 1.</b> Training prompt: A photo of <b>sks</b> urban scene
    </div>
</div>

<!-- We will make experiments on the this image to answer the corresponding questions in below. The training configuration remains as default.<span id="q1"></span> -->

<h4 id="1-what-will-happen-if-the-training-prompt-does-not-indicate-exactly-the-style-of-the-input-">1. What will happen if the training prompt does not indicate exactly the style of the input ?</h4>

<p>The investigation aims to assess the impact of invalid training prompts on the quality of the fine-tuned model. Our findings confirm that such prompts indeed have a detrimental effect. Specifically, when considering the class “rainy,” the majority of the generated images fail to depict rain accurately. In the case of the classes “snowy” and “sunny,” although the outputs appear satisfactory overall, noticeable deficiencies arise with respect to the representation of people and cars, as these elements tend to be diminished or absent.</p>

<!-- We would like to see if the invalid training prompt affects the quality of the fine-tuned model. The answer is yes. Most of the generated images with respect to the class rainy do not contain the rain. For the classes <i>snowy</i> and <i>sunny</i>, the outputs seems good but people and cars almost disappear. -->

<div>
    <div>
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_rainy_day.png" width="49%">
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_rainy_day_sky.png" width="49%">
        <div class="img-cap">
            <b>Figure 2.</b> Test prompt: A photo of <b>sks</b> urban scene on a <i>rainy</i> day. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 200 iterations
        </div>
    </div>
</div>

<div>
    <div>
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_sunny_day.png" width="49%">
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_sunny_day_sky.png" width="49%">
        <div class="img-cap">
            <b>Figure 3.</b> Test prompt: A photo of <b>sks</b> urban scene on a <i>sunny</i> day. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 200 iterations
        </div>
    </div>
</div>

<div>
    <div>
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_snowy_day.png" width="49%">
        <img src="/assets/img/news/DATUM/a_photo_of_sks_urban_scene_on_a_snowy_day_sky.png" width="49%">
        <div class="img-cap">
            <b>Figure 4.</b> Test prompt: A photo of <b>sks</b> urban scene on a <i>snowy</i> day. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 200 iterations
        </div>
    </div>
</div>

<p>These results underscore the significance of carefully selecting the training prompts during the fine-tuning process of the DreamBooth model. They align with the conclusions drawn in the ablation study conducted by Benigmim et al. <a href="#top">(DATUM, 2023)</a>, which also highlights the influence of training prompts on segmentation outcomes. However, it is worth noting that the endeavor of prompt engineering entails considerable effort in order to identify an optimal training text that yields desirable results in image sampling. Fortunately, Bao et al. have introduced a solution <a href="#top">(UniDiffuser, 2023)</a> that leverages a variant of the transformer model. This approach can be effectively employed to deduce an appropriate prompt for input images in text-to-image models such as StableDiffusion and DreamBooth.</p>

<p><!-- These results show that when fine-tuning DreamBooth, we should select the training prompt carefully otherwise we end up with unexpected cases. It is also shown in the ablation study by Benigmim et al. <a href="#top">(DATUM, 2023)</a> on how the training prompts impact the segmentation results. However, it requires a huge effort on prompt engineering to actually choose an optimal training text that performs well on sampling. Fortunately, Bao et al. propose <a href="#top">(UniDiffuser, 2023)</a> which employs a variation of transformer and can be exploited to infer a correct prompt of an input image for text-to-image models as StableDiffusion and DreamBooth.  --></p>

<h4 id="2-how-does-the-model-work-if-the-classes-are-not-relevant-to-the-style-">2. How does the model work if the classes are not relevant to the style ?</h4>
<!-- When the classes are not relevant to the style (e.g. "paddy field", "moutain"), the fine-tuned model with the correct prompt is still able to generate valid images while the one with invalid prompt loses its generalized capability. -->
<p>In scenarios where the targeted classes do not align with the intended style (e.g., “paddy field” or “mountain”), it is observed that the fine-tuned model incorporating the correct prompt retains its ability to generate valid images. Conversely, the model utilizing an invalid prompt demonstrates a diminished capacity for generalization.</p>

<div>
    <div>
        <img src="/assets/img/news/DATUM/a_photo_of_sks_class.png" width="49%">
        <img src="/assets/img/news/DATUM/a_photo_of_sks_class_sky.png" width="49%">
        <div class="img-cap">
            <b>Figure 5.</b> Test prompt: A photo of <b>sks</b> &lt;class&gt;.  &lt;class&gt; from left to right: urban scene - paddy field - mountain - sky. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 200 iterations
        </div>
    </div>
</div>
<!-- The results in Figure 5 show that if we would like to sample images that are not really related to the style, e.g. <span style="color:red">paddy field</span> and <span style="color:red">mountain</span> versus <span style="color:cyan">sky</span> and <span style="color:cyan">urban scene</span>, we are more likely to get the images relevant to the training prompt than the test prompt. Hence, the prompt is sensitive not only for the style (fine-tuning phase) as in Question <a href="#q1">1</a> above but also for the target (testing phase) that we opt to. We notice that for users to attentively input prompts and use valid fine-tuned models. -->
<p>Figure 5 depicts the results indicating that when attempting to sample images unrelated to the style, such as <span style="color:red">paddy field</span> and <span style="color:red">mountain</span> compared to <span style="color:cyan">sky</span> and <span style="color:cyan">urban scene</span>, the generated images are more likely to correspond to the training prompt rather than the desired test prompt. This observation underscores the sensitivity of the prompt, not only in the fine-tuning phase concerning style (as addressed in Question <a href="#q1">1</a>), but also in the testing phase regarding the specific target prompt chosen. Consequently, it is imperative for users to exercise caution when providing prompts and ensure the utilization of valid and appropriate fine-tuned models to obtain desired outcomes.</p>

<hr>

<h4 id="3-try-to-synthesize-images-in-different-weather-conditions">3. Try to synthesize images in different weather conditions</h4>
<div>
    <img src="/assets/img/news/DATUM/sks-urban-scene-rainy-day.jpg" width="100%">
    <div class="img-cap">
        <b>Figure 6.</b> Training prompt: A photo of <b>sks</b> urban scene on a rainy day
    </div>
</div>
<!-- Training the following image under rainy condition. -->
<p>We expand our experiments on a different image as illustrated in Figure 6, where the weather condition “rainy” is given. We would like to expect if we can replace “rainy” by other conditions as “sunny” and “snowy”. Additionally, we intend to explore the synthesis of images depicting different weather conditions by modifying the number of iterations during the training phase, specifically employing 200 and 1000 iterations. This modification aims to investigate whether, over time, we can generate images of the same scene with varying weather conditions. By incorporating these adjustments, we aspire to gain further insights into the behavior and capabilities of the model within different experimental scenarios.</p>
<div>
    <img src="/assets/img/news/DATUM/sks-urban-scene-weather-200.png" width="100%">
    <div class="img-cap">
        <b>Figure 7.</b> Test prompt: A photo of <b>sks</b> &lt;class&gt;.  &lt;class&gt; from top to bottom: sunny - rainy - snowy. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 200 iterations
    </div>
</div>
<!-- The results in Figure 7 show that we can definitely use DATUM to adapt the images in "rainy" to "sunny", "snowy" or different style of "rainy". However, we can also see that the fine-tuned model learns the piles as a style. -->
<p>The findings presented in Figure 7 definitely demonstrate the effectiveness of DATUM in facilitating the adaptation of images from the “rainy” domain to alternative styles such as “sunny”, “snowy”, or even varying manifestations of the “rainy” scene. Nevertheless, an intriguing observation is made, revealing that the fine-tuned model exhibits a propensity for treating the visual concept of “piles” as an inherent stylistic attribute. One can try to fine-tune DreamBooth in smaller numbers of iterations to see if “piles” disappear without negatively affect the different weather conditions synthesis.</p>
<div>
    <img src="/assets/img/news/DATUM/sks-urban-scene-weather-1000.png" width="100%">
    <div class="img-cap">
        <b>Figure 8.</b> Test prompt: A photo of <b>sks</b> &lt;class&gt;.  &lt;class&gt; from top to bottom: sunny - rainy - snowy. Training prompt: "A photo of <b>sks</b> urban scene" (Left) vs "A photo of <b>sks</b> sky" (Right). Fine-tune in 1000 iterations
    </div>
</div>
<!-- When train with 1000 iterations, the model overfits and cannot generate images with respect to <i>sunny</i> and <i>snowy</i>. Moreover, the <i>rainy</i>, <i>piles</i>, <i>purple color</i>, <i>people riding bikes with raincoat</i>, etc. are also considered to be the style. -->
<p>When the model is trained using 1000 iterations, a phenomenon of overfitting is observed, resulting in the model’s inability to generate images accurately corresponding to the classes of “sunny” and “snowy”. Furthermore, it is worth noting that the attributes of “rainy,” “piles,” “purple color,” “people riding bikes with raincoat,” and others are also deemed stylistic elements within the training framework.</p>

    </div>
  </article>
</div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Thuy Vinh Dinh Tran. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.`.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
